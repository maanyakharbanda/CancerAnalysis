{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 128, Epochs: 200\n",
      "Augmentation factor: 5x\n",
      "\n",
      "\n",
      "==================================================\n",
      "Creating training dataset with augmentation:\n",
      "Dataset: train.csv\n",
      "  Original samples: 728\n",
      "  Positive (cancer): 393 (54.0%)\n",
      "  Negative (healthy): 335 (46.0%)\n",
      "  All image files verified successfully\n",
      "  Augmented training size: 3640 images (5x original)\n",
      "\n",
      "==================================================\n",
      "Creating validation dataset:\n",
      "Dataset: val.csv\n",
      "  Original samples: 208\n",
      "  Positive (cancer): 116 (55.8%)\n",
      "  Negative (healthy): 92 (44.2%)\n",
      "  All image files verified successfully\n",
      "  Validation size: 208 images\n",
      "\n",
      "==================================================\n",
      "Training batches: 29 (3640 images)\n",
      "Validation batches: 2 (208 images)\n",
      "\n",
      "==================================================\n",
      "Initializing ResNet50 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/anaconda3/envs/mark_7/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model architecture:\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Trainable parameters: 23,510,081\n",
      "\n",
      "Visualizing augmented images...\n",
      "Saved augmentation_samples.png\n",
      "\n",
      "==================================================\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/200 (21.7s)\n",
      "  Train Loss: 0.1464\n",
      "  Val Loss: 0.9354 | Val Acc: 0.8990\n",
      "  Current LR: 0.001000\n",
      "  Saved new best model with accuracy: 0.8990\n",
      "\n",
      "Epoch 2/200 (21.8s)\n",
      "  Train Loss: 0.0529\n",
      "  Val Loss: 0.1308 | Val Acc: 0.9760\n",
      "  Current LR: 0.001000\n",
      "  Saved new best model with accuracy: 0.9760\n",
      "\n",
      "Epoch 3/200 (21.7s)\n",
      "  Train Loss: 0.0216\n",
      "  Val Loss: 0.0201 | Val Acc: 0.9904\n",
      "  Current LR: 0.001000\n",
      "  Saved new best model with accuracy: 0.9904\n",
      "\n",
      "Epoch 4/200 (21.8s)\n",
      "  Train Loss: 0.0142\n",
      "  Val Loss: 0.0886 | Val Acc: 0.9856\n",
      "  Current LR: 0.001000\n",
      "\n",
      "Epoch 5/200 (21.7s)\n",
      "  Train Loss: 0.0192\n",
      "  Val Loss: 0.2567 | Val Acc: 0.9519\n",
      "  Current LR: 0.001000\n",
      "\n",
      "Epoch 6/200 (21.9s)\n",
      "  Train Loss: 0.0166\n",
      "  Val Loss: 0.0244 | Val Acc: 0.9904\n",
      "  Current LR: 0.001000\n",
      "\n",
      "Epoch 7/200 (21.6s)\n",
      "  Train Loss: 0.0108\n",
      "  Val Loss: 0.0567 | Val Acc: 0.9904\n",
      "  Current LR: 0.000500\n",
      "\n",
      "Epoch 8/200 (21.9s)\n",
      "  Train Loss: 0.0059\n",
      "  Val Loss: 0.0278 | Val Acc: 0.9904\n",
      "  Current LR: 0.000500\n",
      "\n",
      "Epoch 9/200 (21.9s)\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0033 | Val Acc: 1.0000\n",
      "  Current LR: 0.000500\n",
      "  Saved new best model with accuracy: 1.0000\n",
      "\n",
      "Epoch 10/200 (21.9s)\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0398 | Val Acc: 0.9904\n",
      "  Current LR: 0.000500\n",
      "\n",
      "Epoch 11/200 (21.9s)\n",
      "  Train Loss: 0.0125\n",
      "  Val Loss: 0.0382 | Val Acc: 0.9856\n",
      "  Current LR: 0.000500\n",
      "\n",
      "Epoch 12/200 (22.1s)\n",
      "  Train Loss: 0.0047\n",
      "  Val Loss: 0.0553 | Val Acc: 0.9904\n",
      "  Current LR: 0.000500\n",
      "\n",
      "Epoch 13/200 (22.1s)\n",
      "  Train Loss: 0.0019\n",
      "  Val Loss: 0.0624 | Val Acc: 0.9856\n",
      "  Current LR: 0.000250\n",
      "\n",
      "Epoch 14/200 (22.1s)\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0300 | Val Acc: 0.9904\n",
      "  Current LR: 0.000250\n",
      "\n",
      "Epoch 15/200 (22.1s)\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0325 | Val Acc: 0.9904\n",
      "  Current LR: 0.000250\n",
      "\n",
      "Epoch 16/200 (22.1s)\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0358 | Val Acc: 0.9904\n",
      "  Current LR: 0.000250\n",
      "\n",
      "Epoch 17/200 (21.7s)\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0351 | Val Acc: 0.9904\n",
      "  Current LR: 0.000125\n",
      "\n",
      "Epoch 18/200 (21.9s)\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0344 | Val Acc: 0.9904\n",
      "  Current LR: 0.000125\n",
      "\n",
      "Epoch 19/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0369 | Val Acc: 0.9904\n",
      "  Current LR: 0.000125\n",
      "\n",
      "Epoch 20/200 (21.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0373 | Val Acc: 0.9904\n",
      "  Current LR: 0.000125\n",
      "\n",
      "Epoch 21/200 (22.5s)\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0337 | Val Acc: 0.9904\n",
      "  Current LR: 0.000063\n",
      "\n",
      "Epoch 22/200 (22.5s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0324 | Val Acc: 0.9904\n",
      "  Current LR: 0.000063\n",
      "\n",
      "Epoch 23/200 (22.1s)\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0329 | Val Acc: 0.9904\n",
      "  Current LR: 0.000063\n",
      "\n",
      "Epoch 24/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0348 | Val Acc: 0.9904\n",
      "  Current LR: 0.000063\n",
      "\n",
      "Epoch 25/200 (22.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0367 | Val Acc: 0.9904\n",
      "  Current LR: 0.000031\n",
      "\n",
      "Epoch 26/200 (21.8s)\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0341 | Val Acc: 0.9904\n",
      "  Current LR: 0.000031\n",
      "\n",
      "Epoch 27/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0355 | Val Acc: 0.9904\n",
      "  Current LR: 0.000031\n",
      "\n",
      "Epoch 28/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0365 | Val Acc: 0.9904\n",
      "  Current LR: 0.000031\n",
      "\n",
      "Epoch 29/200 (22.0s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0350 | Val Acc: 0.9904\n",
      "  Current LR: 0.000016\n",
      "\n",
      "Epoch 30/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0369 | Val Acc: 0.9904\n",
      "  Current LR: 0.000016\n",
      "\n",
      "Epoch 31/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0344 | Val Acc: 0.9904\n",
      "  Current LR: 0.000016\n",
      "\n",
      "Epoch 32/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0376 | Val Acc: 0.9904\n",
      "  Current LR: 0.000016\n",
      "\n",
      "Epoch 33/200 (22.3s)\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0350 | Val Acc: 0.9904\n",
      "  Current LR: 0.000008\n",
      "\n",
      "Epoch 34/200 (21.9s)\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0351 | Val Acc: 0.9904\n",
      "  Current LR: 0.000008\n",
      "\n",
      "Epoch 35/200 (21.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0335 | Val Acc: 0.9904\n",
      "  Current LR: 0.000008\n",
      "\n",
      "Epoch 36/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0371 | Val Acc: 0.9904\n",
      "  Current LR: 0.000008\n",
      "\n",
      "Epoch 37/200 (21.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0365 | Val Acc: 0.9904\n",
      "  Current LR: 0.000004\n",
      "\n",
      "Epoch 38/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0357 | Val Acc: 0.9904\n",
      "  Current LR: 0.000004\n",
      "\n",
      "Epoch 39/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0351 | Val Acc: 0.9904\n",
      "  Current LR: 0.000004\n",
      "\n",
      "Epoch 40/200 (21.8s)\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0366 | Val Acc: 0.9904\n",
      "  Current LR: 0.000004\n",
      "\n",
      "Epoch 41/200 (21.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0349 | Val Acc: 0.9904\n",
      "  Current LR: 0.000002\n",
      "\n",
      "Epoch 42/200 (21.8s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0360 | Val Acc: 0.9904\n",
      "  Current LR: 0.000002\n",
      "\n",
      "Epoch 43/200 (21.9s)\n",
      "  Train Loss: 0.0000\n",
      "  Val Loss: 0.0363 | Val Acc: 0.9904\n",
      "  Current LR: 0.000002\n",
      "\n",
      "Epoch 44/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0358 | Val Acc: 0.9904\n",
      "  Current LR: 0.000002\n",
      "\n",
      "Epoch 45/200 (21.9s)\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0356 | Val Acc: 0.9904\n",
      "  Current LR: 0.000001\n",
      "\n",
      "Epoch 46/200 (22.0s)\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0363 | Val Acc: 0.9904\n",
      "  Current LR: 0.000001\n",
      "\n",
      "Epoch 47/200 (22.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0363 | Val Acc: 0.9904\n",
      "  Current LR: 0.000001\n",
      "\n",
      "Epoch 48/200 (22.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0349 | Val Acc: 0.9904\n",
      "  Current LR: 0.000001\n",
      "\n",
      "Epoch 49/200 (22.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0359 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 50/200 (20.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0347 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 51/200 (20.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0357 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 52/200 (20.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0347 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 53/200 (20.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0356 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 54/200 (21.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0365 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 55/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0369 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 56/200 (22.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0349 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 57/200 (22.1s)\n",
      "  Train Loss: 0.0000\n",
      "  Val Loss: 0.0363 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 58/200 (21.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0353 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 59/200 (22.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0357 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 60/200 (22.0s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0359 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 61/200 (22.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0347 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 62/200 (21.3s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0359 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 63/200 (21.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0346 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 64/200 (21.1s)\n",
      "  Train Loss: 0.0000\n",
      "  Val Loss: 0.0360 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 65/200 (20.7s)\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0361 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 66/200 (21.1s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0355 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 67/200 (20.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0357 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 68/200 (20.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0350 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 69/200 (20.6s)\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0359 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 70/200 (20.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0347 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 71/200 (20.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0367 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 72/200 (20.9s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0360 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 73/200 (20.7s)\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: 0.0355 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 74/200 (21.3s)\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0357 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n",
      "\n",
      "Epoch 75/200 (21.5s)\n",
      "  Train Loss: 0.0000\n",
      "  Val Loss: 0.0372 | Val Acc: 0.9904\n",
      "  Current LR: 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 320\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m trained_model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Final summary\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 243\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m    240\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    241\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 243\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Skip batches with invalid labels\u001b[39;49;00m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSkipping batch with invalid labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mark_7/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/mark_7/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mark_7/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1410\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1410\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1412\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/mark_7/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mark_7/lib/python3.12/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/mark_7/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Configuration\n",
    "TRAIN_IMG_DIR = '/media/prashant/12 TB HDD 21/cancer_portal/lung_cancer/latest_multiclass/train'\n",
    "VAL_IMG_DIR = '/media/prashant/12 TB HDD 21/cancer_portal/lung_cancer/latest_multiclass/valid'\n",
    "TRAIN_CSV = '/media/prashant/12 TB HDD 21/cancer_portal/lung_cancer/latest_multiclass/train.csv'\n",
    "VAL_CSV = '/media/prashant/12 TB HDD 21/cancer_portal/lung_cancer/latest_multiclass/val.csv'\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 200\n",
    "TRAIN_AUGMENT_FACTOR = 5  # How many augmented versions to create per original image\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Augmentation factor: {TRAIN_AUGMENT_FACTOR}x\\n\")\n",
    "\n",
    "# Custom Dataset Class with verification and augmentation\n",
    "class AugmentedLungCancerDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None, augment=False):\n",
    "        # Read and verify CSV\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Verify CSV structure\n",
    "        required_columns = ['filename', 'Nodule']\n",
    "        for col in required_columns:\n",
    "            if col not in self.df.columns:\n",
    "                raise ValueError(f\"CSV missing required column: '{col}'\")\n",
    "        \n",
    "        # Verify labels\n",
    "        invalid_labels = self.df[~self.df['Nodule'].isin([0, 1])]\n",
    "        if not invalid_labels.empty:\n",
    "            raise ValueError(f\"Invalid labels found in CSV:\\n{invalid_labels}\")\n",
    "        \n",
    "        # Verify image directory\n",
    "        if not os.path.exists(img_dir):\n",
    "            raise NotADirectoryError(f\"Image directory not found: {img_dir}\")\n",
    "        \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.original_size = len(self.df)\n",
    "        \n",
    "        # Print dataset stats\n",
    "        num_pos = (self.df['Nodule'] == 1).sum()\n",
    "        num_neg = (self.df['Nodule'] == 0).sum()\n",
    "        print(f\"Dataset: {os.path.basename(csv_path)}\")\n",
    "        print(f\"  Original samples: {self.original_size}\")\n",
    "        print(f\"  Positive (cancer): {num_pos} ({num_pos/self.original_size*100:.1f}%)\")\n",
    "        print(f\"  Negative (healthy): {num_neg} ({num_neg/self.original_size*100:.1f}%)\")\n",
    "        \n",
    "        # Verify image files\n",
    "        missing_files = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            img_path = os.path.join(self.img_dir, row['filename'])\n",
    "            if not os.path.exists(img_path):\n",
    "                missing_files.append((idx, row['filename']))\n",
    "        \n",
    "        if missing_files:\n",
    "            print(\"\\nWARNING: Missing image files:\")\n",
    "            for idx, fname in missing_files[:5]:  # Show first 5 missing files\n",
    "                print(f\"  Row {idx}: {fname}\")\n",
    "            if len(missing_files) > 5:\n",
    "                print(f\"  ... and {len(missing_files)-5} more\")\n",
    "            raise FileNotFoundError(f\"Total missing images: {len(missing_files)}\")\n",
    "        else:\n",
    "            print(\"  All image files verified successfully\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.augment:\n",
    "            return self.original_size * TRAIN_AUGMENT_FACTOR\n",
    "        return self.original_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate original index for non-augmented dataset\n",
    "        orig_idx = idx % self.original_size\n",
    "        \n",
    "        img_name = self.df.iloc[orig_idx]['filename']\n",
    "        label = self.df.iloc[orig_idx]['Nodule']\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR loading image: {img_path}\")\n",
    "            print(f\"Exception: {str(e)}\")\n",
    "            # Return blank image as fallback\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "            return image, torch.tensor(-1, dtype=torch.float32)  # Invalid label\n",
    "        \n",
    "        if self.transform:\n",
    "            # Apply different augmentation each time for augmented dataset\n",
    "            if self.augment:\n",
    "                # Create a new random transform for each augmentation\n",
    "                aug_transform = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomRotation(10),\n",
    "                    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                image = aug_transform(image)\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Image Transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create Datasets and DataLoaders with verification\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Creating training dataset with augmentation:\")\n",
    "train_dataset = AugmentedLungCancerDataset(TRAIN_CSV, TRAIN_IMG_DIR, train_transform, augment=True)\n",
    "print(f\"  Augmented training size: {len(train_dataset)} images ({TRAIN_AUGMENT_FACTOR}x original)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Creating validation dataset:\")\n",
    "val_dataset = AugmentedLungCancerDataset(VAL_CSV, VAL_IMG_DIR, val_transform, augment=False)\n",
    "print(f\"  Validation size: {len(val_dataset)} images\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Training batches: {len(train_loader)} ({len(train_dataset)} images)\")\n",
    "print(f\"Validation batches: {len(val_loader)} ({len(val_dataset)} images)\")\n",
    "\n",
    "# Initialize Model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Initializing ResNet50 model...\")\n",
    "model = models.resnet50(weights='DEFAULT')\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTrainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Function to visualize augmented images\n",
    "def visualize_augmentations(dataset, num_samples=5):\n",
    "    print(\"\\nVisualizing augmented images...\")\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples*3))\n",
    "    fig.suptitle('Image Augmentation Examples', fontsize=16)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Show original image\n",
    "        orig_idx = np.random.randint(0, dataset.original_size)\n",
    "        img_name = dataset.df.iloc[orig_idx]['filename']\n",
    "        img_path = os.path.join(dataset.img_dir, img_name)\n",
    "        orig_image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Show two augmented versions\n",
    "        aug_idx1 = orig_idx * TRAIN_AUGMENT_FACTOR\n",
    "        aug_idx2 = orig_idx * TRAIN_AUGMENT_FACTOR + 1\n",
    "        \n",
    "        # Get augmented images directly from dataset\n",
    "        aug_image1, _ = dataset[aug_idx1]\n",
    "        aug_image2, _ = dataset[aug_idx2]\n",
    "        \n",
    "        # Convert tensors to numpy and denormalize\n",
    "        def denormalize(tensor):\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            tensor = tensor * std + mean\n",
    "            tensor = tensor.clamp(0, 1)\n",
    "            return tensor.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Plot images\n",
    "        axes[i, 0].imshow(orig_image)\n",
    "        axes[i, 0].set_title(f\"Original\\n{img_name}\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(denormalize(aug_image1))\n",
    "        axes[i, 1].set_title(\"Augmentation 1\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(denormalize(aug_image2))\n",
    "        axes[i, 2].set_title(\"Augmentation 2\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('augmentation_samples.png', dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved augmentation_samples.png\")\n",
    "\n",
    "# Visualize augmentations\n",
    "visualize_augmentations(train_dataset)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        processed = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # Skip batches with invalid labels\n",
    "            if (labels < 0).any():\n",
    "                print(\"Skipping batch with invalid labels\")\n",
    "                continue\n",
    "                \n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.unsqueeze(1).to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            processed += inputs.size(0)\n",
    "            \n",
    "        epoch_loss = running_loss / processed\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Skip batches with invalid labels\n",
    "                if (labels < 0).any():\n",
    "                    print(\"Skipping batch with invalid labels\")\n",
    "                    continue\n",
    "                    \n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.unsqueeze(1).to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        val_loss = val_loss / total if total > 0 else 0\n",
    "        val_acc = correct / total if total > 0 else 0\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s)\")\n",
    "        print(f\"  Train Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, 'best_resnet50_augmented_model.pth')\n",
    "            print(f\"  Saved new best model with accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining complete in {total_time//60:.0f}m {total_time%60:.0f}s\")\n",
    "    print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "# Start training\n",
    "trained_model, history = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Summary:\")\n",
    "print(f\"Original training samples: {train_dataset.original_size}\")\n",
    "print(f\"Augmented training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_acc']):.4f}\")\n",
    "print(f\"Model saved as: 'best_resnet50_augmented_model.pth'\")\n",
    "\n",
    "# Plot training history\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy', color='green')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved training_history.png\")\n",
    "\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mark_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
